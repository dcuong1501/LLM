import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging

# Thiáº¿t láº­p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_model_and_tokenizer(model_path, base_model_name="microsoft/Phi-3-mini-4k-instruct"):
    """Táº£i mÃ´ hÃ¬nh vÃ  tokenizer."""
    logger.info("ğŸ“¥ Äang táº£i tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)

    # Äáº·t pad_token náº¿u chÆ°a cÃ³
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info("ğŸ”§ ÄÃ£ Ä‘áº·t pad_token báº±ng eos_token")

    logger.info("ğŸ“¥ Äang táº£i mÃ´ hÃ¬nh gá»‘c...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,  # Sá»­ dá»¥ng float32 cho CPU
        device_map="cpu",  # Ã‰p cháº¡y trÃªn CPU
        attn_implementation="eager"
    )

    logger.info("ğŸ“¥ Äang táº£i mÃ´ hÃ¬nh LoRA Ä‘Ã£ tinh chá»‰nh...")
    try:
        model = PeftModel.from_pretrained(base_model, model_path)
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh LoRA: {e}")
        raise

    logger.info("âœ… MÃ´ hÃ¬nh vÃ  tokenizer Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng")
    return model, tokenizer


def generate_response(model, tokenizer, prompt, max_length=200):
    """Táº¡o pháº£n há»“i tá»« mÃ´ hÃ¬nh."""
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, return_attention_mask=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_return_sequences=1,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response


def main():
    """Cháº¡y giao diá»‡n chat."""
    model_path = "./songoku-ultimate-results/final-model"
    try:
        model, tokenizer = load_model_and_tokenizer(model_path)
    except Exception as e:
        logger.error(f"âŒ KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh: {e}")
        print("âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh. Kiá»ƒm tra Ä‘Æ°á»ng dáº«n hoáº·c cáº¥u hÃ¬nh.")
        return

    print("\nğŸ‰ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Son Goku AI! ğŸ‰")
    print("HÃ£y nÃ³i chuyá»‡n vá»›i Goku! Nháº­p 'thoÃ¡t' Ä‘á»ƒ káº¿t thÃºc.\n")

    while True:
        user_input = input("ğŸ‘¤ Báº¡n: ")
        if user_input.lower() == "thoÃ¡t":
            print("ğŸ‘‹ Táº¡m biá»‡t! Háº¹n gáº·p láº¡i, chiáº¿n binh!")
            break

        # Táº¡o prompt theo phong cÃ¡ch Son Goku
        prompt = f"[INST] Báº¡n lÃ  Son Goku, má»™t chiáº¿n binh Saiyan máº¡nh máº½ vÃ  nhiá»‡t huyáº¿t. HÃ£y tráº£ lá»i cÃ¢u há»i nÃ y vá»›i phong cÃ¡ch cá»§a Goku, sá»­ dá»¥ng giá»ng Ä‘iá»‡u hÃ o sáº£ng, thÃ¢n thiá»‡n vÃ  sáºµn sÃ ng chiáº¿n Ä‘áº¥u! CÃ¢u há»i: {user_input} [/INST]\n"
        logger.info(f"ğŸ“© Nháº­n Ä‘Æ°á»£c cÃ¢u há»i: {user_input}")

        try:
            response = generate_response(model, tokenizer, prompt)
            print(f"ğŸ¤– Goku: {response.replace(prompt, '').strip()}\n")
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº¡o pháº£n há»“i: {e}")
            print("âŒ CÃ³ lá»—i xáº£y ra, vui lÃ²ng thá»­ láº¡i!\n")


if __name__ == "__main__":
    main()